{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-13 12:32:23.519\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtweedejaars_project.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mPROJ_ROOT path is: /home/nordin/Desktop/project/tweedejaars_project\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from tweedejaars_project import *\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.tree import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'import_capacity',\n",
    "    # 'min_price_published',\n",
    "    'mid_price_published',\n",
    "    # 'max_price_published',\n",
    "    # 'min_ptu_price_known',\n",
    "    # 'max_ptu_price_known',\n",
    "    # 'settlement_price_bestguess',\n",
    "    'time_since_last_two_sided',\n",
    "    'two_sided_daily_count',\n",
    "    'PTU',\n",
    "    'naive_strategy_action',\n",
    "    'forecast_wind',\n",
    "    'forecast_solar',\n",
    "    'forecast_demand',\n",
    "    'ptu_id',\n",
    "    'fix_two_sided_ptu_realtime'\n",
    "]\n",
    "\n",
    "# already used\n",
    "target = 'target_two_sided_ptu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_df()\n",
    "splits = get_splits(df, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0            0\n",
      "1            0\n",
      "2            0\n",
      "3            0\n",
      "4            0\n",
      "          ... \n",
      "123835    8255\n",
      "123836    8255\n",
      "123837    8255\n",
      "123838    8255\n",
      "123839    8255\n",
      "Name: ptu_id, Length: 123840, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "sequence_length = 5 # Dit is het PTU aantal dus ptu_id's niet row aantal\n",
    "batch_size = 15000\n",
    "input_size = len(features)\n",
    "hidden_size = 16\n",
    "num_layers = 2\n",
    "output_size = 1\n",
    "num_epochs = 1\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "# Split the data in vars\n",
    "train_data = splits['train']\n",
    "# test_data = splits['test']\n",
    "\n",
    "rows = 20\n",
    "print(train_data['in']['ptu_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, seq_length, ptu_length=15, nan_val1 = 100000, nan_val2 = -100000):\n",
    "        self.data_in = data['in'].astype(np.float32)\n",
    "        self.data_out = data['out'].astype(np.float32)\n",
    "        self.seq_length = seq_length\n",
    "        self.ptu_length = ptu_length\n",
    "        self.max_length = (self.seq_length * self.ptu_length)\n",
    "        self.data_in_padded = self.create_padding()\n",
    "\n",
    "        self.sequence_indices = self.create_sequences()\n",
    "\n",
    "        # # DIT IS EEN HACK DUS WSS VERANDEREN\n",
    "        # self.data_in = self.data_in.fillna({'min_price_published':nan_val1, 'max_price_published':nan_val2}).astype(np.float32)\n",
    "\n",
    "    def create_padding(self, padding_value=np.nan):\n",
    "        new_rows = pd.DataFrame([[padding_value] * len(self.data_in.columns)] * self.max_length, columns=self.data_in.columns)\n",
    "        return pd.concat([new_rows, self.data_in], ignore_index=True)\n",
    "\n",
    "    def create_sequences(self):\n",
    "        sequence_indices = []        \n",
    "        first_idx = 0\n",
    "        last_idx = self.max_length\n",
    "\n",
    "        counter = self.ptu_length\n",
    "        for _ in range(len(self.data_in)):\n",
    "            sequence_indices.append((first_idx, last_idx))\n",
    "            counter -= 1\n",
    "            if counter == 0:\n",
    "                first_idx += self.ptu_length\n",
    "                counter = self.ptu_length\n",
    "            \n",
    "            last_idx += 1\n",
    "\n",
    "        return sequence_indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequence_indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start_idx, end_idx = self.sequence_indices[idx]\n",
    "        \n",
    "        # np array sequence and target\n",
    "        sequence = self.data_in_padded.iloc[start_idx:end_idx].values\n",
    "        target = self.data_out.iloc[idx]\n",
    "\n",
    "        sequence = torch.tensor(sequence, dtype=torch.float32)        \n",
    "        current_seq_len = self.ptu_length - ((end_idx - start_idx) % self.ptu_length)\n",
    "        corrected_sequence = F.pad(sequence, (0, 0, current_seq_len, 0), mode='constant', value=np.nan)\n",
    "                \n",
    "        return corrected_sequence, torch.tensor(target, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1,  1],\n",
      "         [ 2,  2],\n",
      "         [ 3,  3],\n",
      "         [ 4,  4],\n",
      "         [ 5,  5]],\n",
      "\n",
      "        [[10, 10],\n",
      "         [20, 20],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0]]])\n",
      "\n",
      "tensor([[[ 1,  1],\n",
      "         [ 2,  2],\n",
      "         [ 3,  3],\n",
      "         [ 4,  4],\n",
      "         [ 5,  5]],\n",
      "\n",
      "        [[10, 10],\n",
      "         [20, 20],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[ 0.0827, -0.0810,  0.0157],\n",
       "        [ 0.3794, -0.4931, -0.1602],\n",
       "        [ 0.1901, -0.1844,  0.0018],\n",
       "        [ 0.4819, -0.6494, -0.2332],\n",
       "        [ 0.2595, -0.2653, -0.0329],\n",
       "        [ 0.2934, -0.3286, -0.0866],\n",
       "        [ 0.3108, -0.3795, -0.1488]], grad_fn=<CatBackward0>), batch_sizes=tensor([2, 2, 1, 1, 1]), sorted_indices=None, unsorted_indices=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "seq_batch = [torch.tensor([[1, 1],\n",
    "                           [2, 2],\n",
    "                           [3, 3],\n",
    "                           [4, 4],\n",
    "                           [5, 5]]),\n",
    "             torch.tensor([[10, 10],\n",
    "                           [20, 20]])]\n",
    "\n",
    "seq_lens = [5, 2]\n",
    "padded_seq_batch = torch.nn.utils.rnn.pad_sequence(seq_batch, batch_first=True)\n",
    "print(padded_seq_batch)\n",
    "print('')\n",
    "packed_seq_batch = torch.nn.utils.rnn.pack_padded_sequence(padded_seq_batch, lengths=seq_lens, batch_first=True)\n",
    "print(padded_seq_batch)\n",
    "lstm = nn.LSTM(input_size=2, hidden_size=3, batch_first=True)\n",
    "output, (hn, cn) = lstm(packed_seq_batch.float()) # pass float tensor instead long tensor.\n",
    "output\n",
    "# padded_output, output_lens = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True, total_length=5)\n",
    "# padded_output, output_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 4, 0],\n",
      "        [5, 6, 7, 0, 0],\n",
      "        [8, 9, 0, 0, 0]])\n",
      "Padded sequences:\n",
      " tensor([[1, 2, 3, 4, 0],\n",
      "        [5, 6, 7, 0, 0],\n",
      "        [8, 9, 0, 0, 0]])\n",
      "Packed sequences:\n",
      " PackedSequence(data=tensor([1, 5, 8, 2, 6, 9, 3, 7, 4]), batch_sizes=tensor([3, 3, 2, 1]), sorted_indices=None, unsorted_indices=None)\n"
     ]
    }
   ],
   "source": [
    "seq1 = torch.tensor([1, 2, 3, 4])    # Length 4\n",
    "seq2 = torch.tensor([5, 6, 7])       # Length 3\n",
    "seq3 = torch.tensor([8, 9])          # Length 2\n",
    "\n",
    "# List of sequences\n",
    "sequences = [seq1, seq2, seq3]\n",
    "\n",
    "# Desired fixed length to pad sequences to\n",
    "desired_length = 5\n",
    "\n",
    "# Pad sequences to the desired length\n",
    "padded_sequences = []\n",
    "lengths = []\n",
    "\n",
    "for seq in sequences:\n",
    "    length = len(seq)\n",
    "    lengths.append(length)\n",
    "    if length < desired_length:\n",
    "        # Pad sequence with zeros (or another value if needed)\n",
    "        padded_seq = F.pad(seq, (0, desired_length - length), \"constant\", 0)\n",
    "    else:\n",
    "        padded_seq = seq\n",
    "    padded_sequences.append(padded_seq)\n",
    "\n",
    "# Stack the padded sequences into a single tensor\n",
    "padded_sequences = torch.stack(padded_sequences)\n",
    "print(padded_sequences)\n",
    "\n",
    "# Convert lengths to a tensor and sort them in descending order\n",
    "lengths = torch.tensor(lengths)\n",
    "lengths, perm_idx = lengths.sort(0, descending=True)\n",
    "padded_sequences = padded_sequences[perm_idx]\n",
    "\n",
    "# Pack the padded sequences\n",
    "packed_sequences = pack_padded_sequence(padded_sequences, lengths, batch_first=True, enforce_sorted=True)\n",
    "\n",
    "print(\"Padded sequences:\\n\", padded_sequences)\n",
    "print(\"Packed sequences:\\n\", packed_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        packed_input = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, _ = self.rnn(packed_input)\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        # Use the last valid output for each sequence\n",
    "        idx = (lengths - 1).view(-1, 1).expand(output.size(0), output.size(2)).unsqueeze(1)\n",
    "        output = output.gather(1, idx).squeeze(1)\n",
    "        \n",
    "        output = self.fc(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_loader(data, sequence_length, batch_size):\n",
    "    dataset = TimeSeriesDataset(data, sequence_length)\n",
    "    # all_sequences, all_targets = dataset.get_all_sequences()\n",
    "    print('done')\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4612/3523582104.py:17: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat([new_rows, self.data_in], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "train_loader = prepare_data_loader(train_data, sequence_length, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([15000, 90, 11])\n",
      "Labels batch shape: torch.Size([15000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.3769e+03, 8.6870e+01, 1.9067e+01, 1.0000e+00, 6.9000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4695e+04, 9.2800e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.6870e+01, 1.9133e+01, 1.0000e+00, 6.9000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4695e+04, 9.2800e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 1.9200e+01, 1.0000e+00, 6.9000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4695e+04, 9.2800e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 1.9267e+01, 1.0000e+00, 6.9000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4695e+04, 9.2800e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 1.9333e+01, 1.0000e+00, 6.9000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4695e+04, 9.2800e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 1.9400e+01, 1.0000e+00, 6.9000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4695e+04, 9.2800e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 1.9467e+01, 1.0000e+00, 6.9000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4695e+04, 9.2800e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 1.9533e+01, 1.0000e+00, 6.9000e+01, 1.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4695e+04, 9.2800e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 1.9600e+01, 1.0000e+00, 6.9000e+01, 1.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4695e+04, 9.2800e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 1.9667e+01, 1.0000e+00, 6.9000e+01, 1.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4695e+04, 9.2800e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 1.9733e+01, 1.0000e+00, 6.9000e+01, 1.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4695e+04, 9.2800e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 1.9800e+01, 1.0000e+00, 6.9000e+01, 1.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4695e+04, 9.2800e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 1.9867e+01, 1.0000e+00, 6.9000e+01, 1.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4695e+04, 9.2800e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 1.9933e+01, 1.0000e+00, 6.9000e+01, 1.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4695e+04, 9.2800e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 0.0000e+00, 1.0000e+00, 6.9000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4695e+04, 9.2800e+02, 1.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 6.6667e-02, 2.0000e+00, 7.0000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4826e+04, 9.2900e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 1.3333e-01, 2.0000e+00, 7.0000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4826e+04, 9.2900e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 2.0000e-01, 2.0000e+00, 7.0000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4826e+04, 9.2900e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 2.6667e-01, 2.0000e+00, 7.0000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4826e+04, 9.2900e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 3.3333e-01, 2.0000e+00, 7.0000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4826e+04, 9.2900e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 4.0000e-01, 2.0000e+00, 7.0000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4826e+04, 9.2900e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 4.6667e-01, 2.0000e+00, 7.0000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4826e+04, 9.2900e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 5.3333e-01, 2.0000e+00, 7.0000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4826e+04, 9.2900e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 6.0000e-01, 2.0000e+00, 7.0000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4826e+04, 9.2900e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 6.6667e-01, 2.0000e+00, 7.0000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4826e+04, 9.2900e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 7.3333e-01, 2.0000e+00, 7.0000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4826e+04, 9.2900e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 8.0000e-01, 2.0000e+00, 7.0000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4826e+04, 9.2900e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 8.6667e-01, 2.0000e+00, 7.0000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4826e+04, 9.2900e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 9.3333e-01, 2.0000e+00, 7.0000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4826e+04, 9.2900e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 1.0000e+00, 2.0000e+00, 7.0000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4826e+04, 9.2900e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 1.0667e+00, 2.0000e+00, 7.1000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4880e+04, 9.3000e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 1.1333e+00, 2.0000e+00, 7.1000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4880e+04, 9.3000e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 1.2000e+00, 2.0000e+00, 7.1000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4880e+04, 9.3000e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 1.2667e+00, 2.0000e+00, 7.1000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4880e+04, 9.3000e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 1.3333e+00, 2.0000e+00, 7.1000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4880e+04, 9.3000e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 1.4000e+00, 2.0000e+00, 7.1000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4880e+04, 9.3000e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 1.4667e+00, 2.0000e+00, 7.1000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4880e+04, 9.3000e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 1.5333e+00, 2.0000e+00, 7.1000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4880e+04, 9.3000e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 1.6000e+00, 2.0000e+00, 7.1000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4880e+04, 9.3000e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 1.6667e+00, 2.0000e+00, 7.1000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4880e+04, 9.3000e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 1.7333e+00, 2.0000e+00, 7.1000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4880e+04, 9.3000e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 1.8000e+00, 2.0000e+00, 7.1000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4880e+04, 9.3000e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 1.8667e+00, 2.0000e+00, 7.1000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4880e+04, 9.3000e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 1.9333e+00, 2.0000e+00, 7.1000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4880e+04, 9.3000e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 2.0000e+00, 2.0000e+00, 7.1000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4880e+04, 9.3000e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 2.0667e+00, 2.0000e+00, 7.2000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4857e+04, 9.3100e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 2.1333e+00, 2.0000e+00, 7.2000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4857e+04, 9.3100e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 2.2000e+00, 2.0000e+00, 7.2000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4857e+04, 9.3100e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 2.2667e+00, 2.0000e+00, 7.2000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4857e+04, 9.3100e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 2.3333e+00, 2.0000e+00, 7.2000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4857e+04, 9.3100e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 2.4000e+00, 2.0000e+00, 7.2000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4857e+04, 9.3100e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 2.4667e+00, 2.0000e+00, 7.2000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4857e+04, 9.3100e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 2.5333e+00, 2.0000e+00, 7.2000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4857e+04, 9.3100e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 2.6000e+00, 2.0000e+00, 7.2000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4857e+04, 9.3100e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 2.6667e+00, 2.0000e+00, 7.2000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4857e+04, 9.3100e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 2.7333e+00, 2.0000e+00, 7.2000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4857e+04, 9.3100e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 2.8000e+00, 2.0000e+00, 7.2000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4857e+04, 9.3100e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 2.8667e+00, 2.0000e+00, 7.2000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4857e+04, 9.3100e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 2.9333e+00, 2.0000e+00, 7.2000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4857e+04, 9.3100e+02, 0.0000e+00],\n",
       "        [1.3769e+03, 8.5090e+01, 3.0000e+00, 2.0000e+00, 7.2000e+01, 0.0000e+00,\n",
       "         4.9066e+03, 0.0000e+00, 1.4857e+04, 9.3100e+02, 0.0000e+00],\n",
       "        [1.1600e+03, 8.5090e+01, 3.0667e+00, 2.0000e+00, 7.3000e+01, 0.0000e+00,\n",
       "         4.9197e+03, 0.0000e+00, 1.4748e+04, 9.3200e+02, 0.0000e+00],\n",
       "        [1.1600e+03, 8.5090e+01, 3.1333e+00, 2.0000e+00, 7.3000e+01, 0.0000e+00,\n",
       "         4.9197e+03, 0.0000e+00, 1.4748e+04, 9.3200e+02, 0.0000e+00],\n",
       "        [1.1600e+03, 6.5740e+01, 3.2000e+00, 2.0000e+00, 7.3000e+01, 0.0000e+00,\n",
       "         4.9197e+03, 0.0000e+00, 1.4748e+04, 9.3200e+02, 0.0000e+00],\n",
       "        [1.1600e+03, 6.5740e+01, 3.2667e+00, 2.0000e+00, 7.3000e+01, 0.0000e+00,\n",
       "         4.9197e+03, 0.0000e+00, 1.4748e+04, 9.3200e+02, 0.0000e+00],\n",
       "        [1.1600e+03, 6.5740e+01, 3.3333e+00, 2.0000e+00, 7.3000e+01, 0.0000e+00,\n",
       "         4.9197e+03, 0.0000e+00, 1.4748e+04, 9.3200e+02, 0.0000e+00],\n",
       "        [1.1600e+03, 6.5740e+01, 3.4000e+00, 2.0000e+00, 7.3000e+01, 0.0000e+00,\n",
       "         4.9197e+03, 0.0000e+00, 1.4748e+04, 9.3200e+02, 0.0000e+00],\n",
       "        [1.1600e+03, 6.5740e+01, 3.4667e+00, 2.0000e+00, 7.3000e+01, 0.0000e+00,\n",
       "         4.9197e+03, 0.0000e+00, 1.4748e+04, 9.3200e+02, 0.0000e+00],\n",
       "        [1.1600e+03, 6.5740e+01, 3.5333e+00, 2.0000e+00, 7.3000e+01, 0.0000e+00,\n",
       "         4.9197e+03, 0.0000e+00, 1.4748e+04, 9.3200e+02, 0.0000e+00],\n",
       "        [1.1600e+03, 6.5740e+01, 3.6000e+00, 2.0000e+00, 7.3000e+01, 0.0000e+00,\n",
       "         4.9197e+03, 0.0000e+00, 1.4748e+04, 9.3200e+02, 0.0000e+00],\n",
       "        [1.1600e+03, 6.5740e+01, 3.6667e+00, 2.0000e+00, 7.3000e+01, 0.0000e+00,\n",
       "         4.9197e+03, 0.0000e+00, 1.4748e+04, 9.3200e+02, 0.0000e+00],\n",
       "        [1.1600e+03, 6.5740e+01, 3.7333e+00, 2.0000e+00, 7.3000e+01, 0.0000e+00,\n",
       "         4.9197e+03, 0.0000e+00, 1.4748e+04, 9.3200e+02, 0.0000e+00],\n",
       "        [1.1600e+03, 6.5740e+01, 3.8000e+00, 2.0000e+00, 7.3000e+01, 0.0000e+00,\n",
       "         4.9197e+03, 0.0000e+00, 1.4748e+04, 9.3200e+02, 0.0000e+00],\n",
       "        [1.1600e+03, 6.5740e+01, 3.8667e+00, 2.0000e+00, 7.3000e+01, 0.0000e+00,\n",
       "         4.9197e+03, 0.0000e+00, 1.4748e+04, 9.3200e+02, 0.0000e+00],\n",
       "        [1.1600e+03, 6.5740e+01, 3.9333e+00, 2.0000e+00, 7.3000e+01, 0.0000e+00,\n",
       "         4.9197e+03, 0.0000e+00, 1.4748e+04, 9.3200e+02, 0.0000e+00],\n",
       "        [1.1600e+03, 6.5740e+01, 4.0000e+00, 2.0000e+00, 7.3000e+01, 0.0000e+00,\n",
       "         4.9197e+03, 0.0000e+00, 1.4748e+04, 9.3200e+02, 0.0000e+00],\n",
       "        [1.1600e+03, 6.5740e+01, 4.0667e+00, 2.0000e+00, 7.4000e+01, 0.0000e+00,\n",
       "         4.9197e+03, 0.0000e+00, 1.4696e+04, 9.3300e+02, 0.0000e+00],\n",
       "        [1.1600e+03, 6.5740e+01, 4.1333e+00, 2.0000e+00, 7.4000e+01, 0.0000e+00,\n",
       "         4.9197e+03, 0.0000e+00, 1.4696e+04, 9.3300e+02, 0.0000e+00],\n",
       "        [1.1600e+03, 6.5740e+01, 4.2000e+00, 2.0000e+00, 7.4000e+01, 0.0000e+00,\n",
       "         4.9197e+03, 0.0000e+00, 1.4696e+04, 9.3300e+02, 0.0000e+00],\n",
       "        [1.1600e+03, 6.5740e+01, 4.2667e+00, 2.0000e+00, 7.4000e+01, 0.0000e+00,\n",
       "         4.9197e+03, 0.0000e+00, 1.4696e+04, 9.3300e+02, 0.0000e+00],\n",
       "        [1.1600e+03, 6.5740e+01, 4.3333e+00, 2.0000e+00, 7.4000e+01, 0.0000e+00,\n",
       "         4.9197e+03, 0.0000e+00, 1.4696e+04, 9.3300e+02, 0.0000e+00],\n",
       "        [       nan,        nan,        nan,        nan,        nan,        nan,\n",
       "                nan,        nan,        nan,        nan,        nan],\n",
       "        [       nan,        nan,        nan,        nan,        nan,        nan,\n",
       "                nan,        nan,        nan,        nan,        nan],\n",
       "        [       nan,        nan,        nan,        nan,        nan,        nan,\n",
       "                nan,        nan,        nan,        nan,        nan],\n",
       "        [       nan,        nan,        nan,        nan,        nan,        nan,\n",
       "                nan,        nan,        nan,        nan,        nan],\n",
       "        [       nan,        nan,        nan,        nan,        nan,        nan,\n",
       "                nan,        nan,        nan,        nan,        nan],\n",
       "        [       nan,        nan,        nan,        nan,        nan,        nan,\n",
       "                nan,        nan,        nan,        nan,        nan],\n",
       "        [       nan,        nan,        nan,        nan,        nan,        nan,\n",
       "                nan,        nan,        nan,        nan,        nan],\n",
       "        [       nan,        nan,        nan,        nan,        nan,        nan,\n",
       "                nan,        nan,        nan,        nan,        nan],\n",
       "        [       nan,        nan,        nan,        nan,        nan,        nan,\n",
       "                nan,        nan,        nan,        nan,        nan],\n",
       "        [       nan,        nan,        nan,        nan,        nan,        nan,\n",
       "                nan,        nan,        nan,        nan,        nan]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: eerste batch eerste item is puur nans?\n",
    "# TODO: Special NaN value for all inputs of NaN\n",
    "train_features, train_labels = next(iter(train_loader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[14000]\n",
    "label = train_labels[0]\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0\n",
      "Sequence shape: torch.Size([15000, 90, 11])\n",
      "Targets shape: torch.Size([15000])\n",
      "Batch: 1\n",
      "Sequence shape: torch.Size([15000, 90, 11])\n",
      "Targets shape: torch.Size([15000])\n",
      "Batch: 2\n",
      "Sequence shape: torch.Size([15000, 90, 11])\n",
      "Targets shape: torch.Size([15000])\n",
      "Batch: 3\n",
      "Sequence shape: torch.Size([15000, 90, 11])\n",
      "Targets shape: torch.Size([15000])\n",
      "Batch: 4\n",
      "Sequence shape: torch.Size([15000, 90, 11])\n",
      "Targets shape: torch.Size([15000])\n",
      "Batch: 5\n",
      "Sequence shape: torch.Size([15000, 90, 11])\n",
      "Targets shape: torch.Size([15000])\n",
      "Batch: 6\n",
      "Sequence shape: torch.Size([15000, 90, 11])\n",
      "Targets shape: torch.Size([15000])\n",
      "Batch: 7\n",
      "Sequence shape: torch.Size([15000, 90, 11])\n",
      "Targets shape: torch.Size([15000])\n",
      "Batch: 8\n",
      "Sequence shape: torch.Size([3840, 90, 11])\n",
      "Targets shape: torch.Size([3840])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (sequences, targets) in enumerate(train_loader):\n",
    "    print(\"Batch:\", batch_idx)\n",
    "    print(\"Sequence shape:\", sequences.shape)  # Print the shape of the input sequences\n",
    "    print(\"Targets shape:\", targets.shape)  # Print the shape of the targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Sequence shape: torch.Size([15000, 90, 11]), Lengths: tensor([90, 90, 90,  ..., 90, 90, 90])\n",
      "Batch 1, Sequence shape: torch.Size([15000, 90, 11]), Lengths: tensor([90, 90, 90,  ..., 90, 90, 90])\n",
      "Batch 2, Sequence shape: torch.Size([15000, 90, 11]), Lengths: tensor([90, 90, 90,  ..., 90, 90, 90])\n",
      "Batch 3, Sequence shape: torch.Size([15000, 90, 11]), Lengths: tensor([90, 90, 90,  ..., 90, 90, 90])\n",
      "Batch 4, Sequence shape: torch.Size([15000, 90, 11]), Lengths: tensor([90, 90, 90,  ..., 90, 90, 90])\n",
      "Batch 5, Sequence shape: torch.Size([15000, 90, 11]), Lengths: tensor([90, 90, 90,  ..., 90, 90, 90])\n",
      "Batch 6, Sequence shape: torch.Size([15000, 90, 11]), Lengths: tensor([90, 90, 90,  ..., 90, 90, 90])\n",
      "Batch 7, Sequence shape: torch.Size([15000, 90, 11]), Lengths: tensor([90, 90, 90,  ..., 90, 90, 90])\n",
      "Batch 8, Sequence shape: torch.Size([3840, 90, 11]), Lengths: tensor([90, 90, 90,  ..., 90, 90, 90])\n"
     ]
    }
   ],
   "source": [
    "def train_rnn(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, (sequences, targets) in enumerate(train_loader):\n",
    "            # Prepare the data\n",
    "            sequences = sequences.float()\n",
    "            targets = targets.float()\n",
    "            lengths = torch.tensor([len(seq) for seq in sequences])\n",
    "            \n",
    "            # Debugging print statements\n",
    "            print(f\"Batch {batch_idx}, Sequence shape: {sequences.shape}, Lengths: {lengths}\")\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(sequences, lengths)\n",
    "            \n",
    "            # Compute loss\n",
    "            targets = targets.unsqueeze(1)  # Ensure targets have the shape (batch_size, 1)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "model = RNNModel(input_size, hidden_size, num_layers, output_size)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_rnn(model, train_loader, criterion, optimizer, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, 'markov', 'rnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.9553\n"
     ]
    }
   ],
   "source": [
    "def test_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_outputs = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for sequences, targets in data_loader:\n",
    "            sequences = sequences.float()\n",
    "            targets = targets.float()\n",
    "            lengths = torch.tensor([len(seq) for seq in sequences])\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(sequences, lengths)\n",
    "            all_outputs.append(outputs)\n",
    "            all_targets.append(targets)\n",
    "            \n",
    "    return torch.cat(all_outputs), torch.cat(all_targets)\n",
    "\n",
    "loaded_model = load_model('markov', 'rnn')\n",
    "\n",
    "# Test the model on the training set\n",
    "outputs, targets = test_model(loaded_model, train_loader)\n",
    "\n",
    "# Convert outputs to probabilities\n",
    "probabilities = torch.sigmoid(outputs)\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "predictions = (probabilities > 0.5).float()\n",
    "\n",
    "# Evaluate the predictions\n",
    "accuracy = (predictions == targets.unsqueeze(1)).float().mean()\n",
    "print(f'Accuracy on training set: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Tensor.unique of tensor([[nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        ...,\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan]])>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities.unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RNNModel(input_size, hidden_size, num_layers, output_size)\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch_idx, (sequences, targets) in enumerate(train_loader):\n",
    "#     print(\"Batch:\", batch_idx)\n",
    "#     print(\"Sequence shape:\", sequences.shape)  # Print the shape of the input sequences\n",
    "#     print(\"Targets shape:\", targets.shape)  # Print the shape of the targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training loop (placeholder, implement training logic)\n",
    "# for epoch in range(1):\n",
    "#     for sequences, targets in train_loader:\n",
    "#         lengths = [min(len(seq), sequence_length) for seq in sequences]\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(sequences, lengths)\n",
    "#         loss = criterion(outputs, targets)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweedejaars_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
