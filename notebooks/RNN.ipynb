{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adapted from: https://www.kaggle.com/code/kanncaa1/recurrent-neural-network-with-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tweedejaars_project import load_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([66074, 9])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tycho/miniconda3/envs/tweedejaars_project/lib/python3.12/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/home/tycho/miniconda3/envs/tweedejaars_project/lib/python3.12/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'Series.swapaxes' is deprecated and will be removed in a future version. Please use 'Series.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Prepare Dataset \"\"\"\n",
    "\n",
    "# load data\n",
    "df = load_df()\n",
    "# highly_correlated_features = ['naive_strategy_action', 'min_ptu_price_known',\n",
    "#                               'min_price_published', 'forecast_wind',\n",
    "#                               'time_since_last_two_sided',\n",
    "#                               'two_sided_daily_count', 'mid_price_published',\n",
    "#                               'vwap_avg', 'vwap_std', 'vwap_qty_sum',\n",
    "#                               'minute_in_ptu', 'downward_dispatch_published',\n",
    "#                               'settlement_price_bestguess', 'import_capacity',\n",
    "#                               'upward_dispatch_published',\n",
    "#                               'settlement_price_realized', 'forecast_solar']\n",
    "\n",
    "highly_correlated_features = ['naive_strategy_action',\n",
    "                              'time_since_last_two_sided',\n",
    "                              'two_sided_daily_count', 'mid_price_published',\n",
    "                              'vwap_avg', 'vwap_std', 'vwap_qty_sum',\n",
    "                              'minute_in_ptu']\n",
    "\n",
    "\n",
    "# split data into features and target\n",
    "targets_numpy = df['target_two_sided_ptu']\n",
    "features_numpy = df[highly_correlated_features]\n",
    "\n",
    "# train test split\n",
    "# could be replaced with scikit-learn TimeSeriesSplit\n",
    "X_train, X_test = np.split(features_numpy, [int(.7 * len(features_numpy))])\n",
    "y_train, y_test = np.split(targets_numpy, [int(.7 * len(targets_numpy))])\n",
    "\n",
    "# create feature and targets tensor for train and test set.\n",
    "featuresTrain = torch.Tensor(X_train.to_numpy(dtype=np.float64))\n",
    "targetsTrain = torch.Tensor(y_train.to_numpy(dtype=np.float64))\n",
    "\n",
    "featuresTest = torch.Tensor(X_test.to_numpy(dtype=np.float64))\n",
    "targetsTest = torch.Tensor(y_test.to_numpy(dtype=np.float64))\n",
    "\n",
    "# force the right dimension\n",
    "targetsTrain = targetsTrain.unsqueeze(1)\n",
    "targetsTest = targetsTest.unsqueeze(1)\n",
    "\n",
    "# Pytorch train and test sets\n",
    "train = torch.cat((featuresTrain, targetsTrain), dim=1)\n",
    "test = torch.cat((featuresTest, targetsTest), dim=1)\n",
    "\n",
    "print(test.shape)\n",
    "\n",
    "# batch_size, epoch and iteration\n",
    "first_batch_size = 15\n",
    "n_iters = 10000\n",
    "num_epochs = 1 + int(n_iters / (len(X_train) / first_batch_size))\n",
    "\n",
    "# data loader makes batches and iterable\n",
    "train_loader = DataLoader(train, batch_size = first_batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test, batch_size = first_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create RNN Model \"\"\"\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # RNN\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers, batch_first=True,\n",
    "                          nonlinearity='relu')\n",
    "        \n",
    "        # Collection layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        # Range redistribution [0,1]\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.shape[0], self.hidden_dim,\n",
    "                         requires_grad=True)\n",
    "        \n",
    "        try:\n",
    "            h0 = hn\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # One time step\n",
    "        rnn_out, hn = self.rnn(x, h0)\n",
    "        \n",
    "        fc_out = self.fc(rnn_out[:, -1])  # last output\n",
    "\n",
    "        output = self.sigmoid(fc_out)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# Create RNN\n",
    "input_dim =  8    # input dimension\n",
    "hidden_dim = 60   # hidden layer dimension\n",
    "num_layers = 2    # number of hidden layers\n",
    "output_dim = 15   # output dimension\n",
    "seq_length = 15   # the number of time steps in each sequence\n",
    "\n",
    "model = RNNModel(input_dim, hidden_dim, num_layers, output_dim)\n",
    "\n",
    "# SGD Optimizer\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100  Loss: 0.8959591388702393  Accuracy: 71.0 % N_postive: 4.0\n",
      "Iteration: 200  Loss: 0.9403648376464844  Accuracy: 82.53333282470703 % N_postive: 3.0\n",
      "Iteration: 300  Loss: 0.5704194903373718  Accuracy: 83.36666870117188 % N_postive: 3.0\n",
      "Iteration: 400  Loss: 0.8080807328224182  Accuracy: 88.33333587646484 % N_postive: 2.0\n",
      "Iteration: 500  Loss: 0.6989461779594421  Accuracy: 94.9000015258789 % N_postive: 1.0\n",
      "Iteration: 600  Loss: 0.7033393979072571  Accuracy: 98.23333740234375 % N_postive: 0.0\n",
      "Iteration: 700  Loss: 0.6939347982406616  Accuracy: 98.53333282470703 % N_postive: 0.0\n",
      "Iteration: 800  Loss: 0.6994175314903259  Accuracy: 99.66666412353516 % N_postive: 0.0\n",
      "Iteration: 900  Loss: 0.6961520910263062  Accuracy: 95.76667022705078 % N_postive: 0.0\n",
      "Iteration: 1000  Loss: 0.6934558749198914  Accuracy: 99.96666717529297 % N_postive: 0.0\n",
      "Iteration: 1100  Loss: 0.694873034954071  Accuracy: 99.66666412353516 % N_postive: 0.0\n",
      "Iteration: 1200  Loss: 0.6936126351356506  Accuracy: 99.13333129882812 % N_postive: 0.0\n",
      "Iteration: 1300  Loss: 0.6932494044303894  Accuracy: 99.69999694824219 % N_postive: 0.0\n",
      "Iteration: 1400  Loss: 0.6943632960319519  Accuracy: 97.03333282470703 % N_postive: 0.0\n",
      "Iteration: 1500  Loss: 0.6937578916549683  Accuracy: 96.76667022705078 % N_postive: 0.0\n",
      "Iteration: 1600  Loss: 0.6934038996696472  Accuracy: 96.46666717529297 % N_postive: 0.0\n",
      "Iteration: 1700  Loss: 0.6944929957389832  Accuracy: 98.76667022705078 % N_postive: 0.0\n",
      "Iteration: 1800  Loss: 0.6935635209083557  Accuracy: 99.23333740234375 % N_postive: 0.0\n",
      "Iteration: 1900  Loss: 0.6934967637062073  Accuracy: 95.33332824707031 % N_postive: 0.0\n",
      "Iteration: 2000  Loss: 0.6932647228240967  Accuracy: 95.06666564941406 % N_postive: 0.0\n",
      "Iteration: 2100  Loss: 0.69581139087677  Accuracy: 98.69999694824219 % N_postive: 0.0\n",
      "Iteration: 2200  Loss: 0.6937502026557922  Accuracy: 96.66666412353516 % N_postive: 0.0\n",
      "Iteration: 2300  Loss: 0.7055837512016296  Accuracy: 98.73333740234375 % N_postive: 0.0\n",
      "Iteration: 2400  Loss: 0.7172409892082214  Accuracy: 100.0 % N_postive: 0.0\n",
      "Iteration: 2500  Loss: 0.6934746503829956  Accuracy: 98.06666564941406 % N_postive: 0.0\n",
      "Iteration: 2600  Loss: 0.6935481429100037  Accuracy: 99.06666564941406 % N_postive: 0.0\n",
      "Iteration: 2700  Loss: 0.6941883563995361  Accuracy: 99.43333435058594 % N_postive: 0.0\n",
      "Iteration: 2800  Loss: 0.6946950554847717  Accuracy: 96.5 % N_postive: 0.0\n",
      "Iteration: 2900  Loss: 0.7044336199760437  Accuracy: 99.93333435058594 % N_postive: 0.0\n",
      "Iteration: 3000  Loss: 0.6931552886962891  Accuracy: 99.96666717529297 % N_postive: 0.0\n",
      "Iteration: 3100  Loss: 0.6933538913726807  Accuracy: 99.13333129882812 % N_postive: 0.0\n",
      "Iteration: 3200  Loss: 0.6931813955307007  Accuracy: 99.73333740234375 % N_postive: 0.0\n",
      "Iteration: 3300  Loss: 0.6931567788124084  Accuracy: 97.73332977294922 % N_postive: 0.0\n",
      "Iteration: 3400  Loss: 0.6932144165039062  Accuracy: 96.83332824707031 % N_postive: 0.0\n",
      "Iteration: 3500  Loss: 0.6933130025863647  Accuracy: 96.53333282470703 % N_postive: 0.0\n",
      "Iteration: 3600  Loss: 0.6938402652740479  Accuracy: 98.76667022705078 % N_postive: 0.0\n",
      "Iteration: 3700  Loss: 0.6931780576705933  Accuracy: 100.0 % N_postive: 0.0\n",
      "Iteration: 3800  Loss: 0.6933348774909973  Accuracy: 96.4000015258789 % N_postive: 0.0\n",
      "Iteration: 3900  Loss: 0.6931473612785339  Accuracy: 94.56666564941406 % N_postive: 0.0\n",
      "Iteration: 4000  Loss: 0.6931513547897339  Accuracy: 97.66667175292969 % N_postive: 0.0\n",
      "Iteration: 4100  Loss: 0.6931630969047546  Accuracy: 96.66666412353516 % N_postive: 0.0\n",
      "Iteration: 4200  Loss: 0.6932394504547119  Accuracy: 98.73333740234375 % N_postive: 0.0\n",
      "Iteration: 4300  Loss: 0.6940147876739502  Accuracy: 100.0 % N_postive: 0.0\n",
      "Iteration: 4400  Loss: 0.6932212710380554  Accuracy: 98.06666564941406 % N_postive: 0.0\n",
      "Iteration: 4500  Loss: 0.6932078003883362  Accuracy: 99.0999984741211 % N_postive: 0.0\n",
      "Iteration: 4600  Loss: 0.6943930387496948  Accuracy: 99.43333435058594 % N_postive: 0.0\n",
      "Iteration: 4700  Loss: 0.6961143016815186  Accuracy: 96.73332977294922 % N_postive: 0.0\n",
      "Iteration: 4800  Loss: 0.6932477355003357  Accuracy: 100.0 % N_postive: 0.0\n",
      "Iteration: 4900  Loss: 0.6931758522987366  Accuracy: 100.0 % N_postive: 0.0\n",
      "Iteration: 5000  Loss: 0.6931604743003845  Accuracy: 99.13333129882812 % N_postive: 0.0\n",
      "Iteration: 5100  Loss: 0.6933615803718567  Accuracy: 100.0 % N_postive: 0.0\n",
      "Iteration: 5200  Loss: 0.6933830380439758  Accuracy: 97.86666870117188 % N_postive: 0.0\n",
      "Iteration: 5300  Loss: 0.6931660771369934  Accuracy: 96.83332824707031 % N_postive: 0.0\n",
      "Iteration: 5400  Loss: 0.6931478381156921  Accuracy: 96.5999984741211 % N_postive: 0.0\n",
      "Iteration: 5500  Loss: 0.6931474804878235  Accuracy: 98.76667022705078 % N_postive: 0.0\n",
      "Iteration: 5600  Loss: 0.6936123967170715  Accuracy: 100.0 % N_postive: 0.0\n",
      "Iteration: 5700  Loss: 0.6932341456413269  Accuracy: 96.4000015258789 % N_postive: 0.0\n",
      "Iteration: 5800  Loss: 0.6931473016738892  Accuracy: 94.86666870117188 % N_postive: 0.0\n",
      "Iteration: 5900  Loss: 0.6931897401809692  Accuracy: 97.39999389648438 % N_postive: 0.0\n",
      "Iteration: 6000  Loss: 0.6931512951850891  Accuracy: 98.0 % N_postive: 0.0\n",
      "Iteration: 6100  Loss: 0.6931473016738892  Accuracy: 97.39999389648438 % N_postive: 0.0\n",
      "Iteration: 6200  Loss: 0.6931488513946533  Accuracy: 100.0 % N_postive: 0.0\n",
      "Iteration: 6300  Loss: 0.6931530833244324  Accuracy: 98.06666564941406 % N_postive: 0.0\n",
      "Iteration: 6400  Loss: 0.6931505799293518  Accuracy: 99.13333129882812 % N_postive: 0.0\n",
      "Iteration: 6500  Loss: 0.6932004690170288  Accuracy: 99.43333435058594 % N_postive: 0.0\n",
      "Iteration: 6600  Loss: 0.6931477189064026  Accuracy: 97.33333587646484 % N_postive: 0.0\n",
      "Iteration: 6700  Loss: 0.6931477785110474  Accuracy: 99.4000015258789 % N_postive: 0.0\n",
      "Iteration: 6800  Loss: 0.6931489109992981  Accuracy: 100.0 % N_postive: 0.0\n",
      "Iteration: 6900  Loss: 0.6931804418563843  Accuracy: 100.0 % N_postive: 0.0\n",
      "Iteration: 7000  Loss: 0.6931566596031189  Accuracy: 99.13333129882812 % N_postive: 0.0\n",
      "Iteration: 7100  Loss: 0.6931964755058289  Accuracy: 98.66667175292969 % N_postive: 2.0\n",
      "Iteration: 7200  Loss: 0.6931636333465576  Accuracy: 97.39999389648438 % N_postive: 0.0\n",
      "Iteration: 7300  Loss: 0.6931491494178772  Accuracy: 95.23332977294922 % N_postive: 0.0\n",
      "Iteration: 7400  Loss: 0.6932594776153564  Accuracy: 98.76667022705078 % N_postive: 0.0\n",
      "Iteration: 7500  Loss: 0.6934131979942322  Accuracy: 100.0 % N_postive: 0.0\n",
      "Iteration: 7600  Loss: 0.6931495070457458  Accuracy: 96.4000015258789 % N_postive: 0.0\n",
      "Iteration: 7700  Loss: 0.6931512355804443  Accuracy: 95.53333282470703 % N_postive: 0.0\n",
      "Iteration: 7800  Loss: 0.6931478381156921  Accuracy: 97.16667175292969 % N_postive: 0.0\n",
      "Iteration: 7900  Loss: 0.69316166639328  Accuracy: 98.69999694824219 % N_postive: 0.0\n",
      "Iteration: 8000  Loss: 0.6931473016738892  Accuracy: 96.26667022705078 % N_postive: 0.0\n",
      "Iteration: 8100  Loss: 0.6931580901145935  Accuracy: 100.0 % N_postive: 0.0\n",
      "Iteration: 8200  Loss: 0.693146288394928  Accuracy: 98.06666564941406 % N_postive: 0.0\n",
      "Iteration: 8300  Loss: 0.6943320035934448  Accuracy: 99.13333129882812 % N_postive: 0.0\n",
      "Iteration: 8400  Loss: 0.6936875581741333  Accuracy: 99.43333435058594 % N_postive: 0.0\n",
      "Iteration: 8500  Loss: 0.6931473016738892  Accuracy: 98.4000015258789 % N_postive: 0.0\n",
      "Iteration: 8600  Loss: 0.6931474804878235  Accuracy: 98.33333587646484 % N_postive: 0.0\n",
      "Iteration: 8700  Loss: 0.6931473016738892  Accuracy: 100.0 % N_postive: 0.0\n",
      "Iteration: 8800  Loss: 0.6931473016738892  Accuracy: 100.0 % N_postive: 0.0\n",
      "Iteration: 8900  Loss: 0.6931473016738892  Accuracy: 99.13333129882812 % N_postive: 0.0\n",
      "Iteration: 9000  Loss: 0.6931484341621399  Accuracy: 99.4000015258789 % N_postive: 0.0\n",
      "Iteration: 9100  Loss: 0.6931473016738892  Accuracy: 97.63333129882812 % N_postive: 0.0\n",
      "Iteration: 9200  Loss: 0.6931473016738892  Accuracy: 95.80000305175781 % N_postive: 0.0\n",
      "Iteration: 9300  Loss: 0.6961774230003357  Accuracy: 98.0 % N_postive: 0.0\n",
      "Iteration: 9400  Loss: 0.694074809551239  Accuracy: 100.0 % N_postive: 0.0\n",
      "Iteration: 9500  Loss: 0.6931526064872742  Accuracy: 96.4000015258789 % N_postive: 0.0\n",
      "Iteration: 9600  Loss: 0.6931971311569214  Accuracy: 97.16667175292969 % N_postive: 0.0\n",
      "Iteration: 9700  Loss: 0.6931473016738892  Accuracy: 95.63333129882812 % N_postive: 0.0\n",
      "Iteration: 9800  Loss: 0.6931777596473694  Accuracy: 99.13333129882812 % N_postive: 0.0\n",
      "Iteration: 9900  Loss: 0.6931473612785339  Accuracy: 95.83332824707031 % N_postive: 0.0\n",
      "Iteration: 10000  Loss: 0.6933272480964661  Accuracy: 100.0 % N_postive: 0.0\n",
      "Iteration: 10100  Loss: 0.6931548714637756  Accuracy: 98.06666564941406 % N_postive: 0.0\n",
      "Iteration: 10200  Loss: 0.6933871507644653  Accuracy: 99.16666412353516 % N_postive: 0.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Run model \"\"\"\n",
    "\n",
    "batch_size = int(first_batch_size / seq_length) \n",
    "loss_list = []\n",
    "iteration_list = []\n",
    "accuracy_list = []\n",
    "count = 0\n",
    "error = nn.BCEWithLogitsLoss()\n",
    "test_iter = iter(test_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        \n",
    "        # Stop if the (last) batch is not big enough\n",
    "        if batch.shape[0] != 15:\n",
    "            break\n",
    "        \n",
    "        train = batch[:, :-1].reshape((batch_size, seq_length, input_dim))\n",
    "        targets = batch[:, -1:]\n",
    "\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward propagation\n",
    "        model.train()\n",
    "        outputs = model(train)\n",
    "        \n",
    "        # Calculate softmax and cross entropy loss\n",
    "        loss = error(outputs.T, targets)\n",
    "\n",
    "        # Calculating gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "        # Calculate Accuracy\n",
    "        if count % 100 == 0:\n",
    "\n",
    "            correct = 0\n",
    "\n",
    "            # Iterate through 200 batches in test dataset\n",
    "            for _ in range(200):\n",
    "\n",
    "                # load a new batch\n",
    "                batch = next(test_iter)\n",
    "\n",
    "                # the iter will eventually run out, reload it              \n",
    "                if True in torch.isnan(batch):\n",
    "                    test_iter = iter(test_loader)\n",
    "                    batch = next(test_iter)\n",
    "            \n",
    "                # seperate features and target\n",
    "                test_features = batch[:, :-1].reshape((batch_size, seq_length, input_dim))\n",
    "                test_targets = batch[:,-1:]\n",
    "                \n",
    "                # Forward propagation\n",
    "                outputs = model(test_features)\n",
    "                \n",
    "                # Round predictions to get final prediction\n",
    "                predicted = torch.round(outputs.T)\n",
    "                \n",
    "                # print(predicted, test_targets)\n",
    "                correct += (predicted == test_targets).sum()\n",
    "                \n",
    "            total = batch.shape[0]*200\n",
    "            accuracy = (correct / float(total)) * 100\n",
    "            positives = predicted.sum()\n",
    "                \n",
    "            # store loss and iteration\n",
    "            iteration_list.append(count)\n",
    "            loss_list.append(loss)\n",
    "            accuracy_list.append(accuracy)\n",
    "\n",
    "            # Print results\n",
    "            print(f'Iteration: {count}  Loss: {loss}  Accuracy: {accuracy} % N_postive: {positives}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Model Evaluation '"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Model Evaluation \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweedejaars_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
