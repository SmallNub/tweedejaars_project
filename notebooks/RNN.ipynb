{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adapted from: https://www.kaggle.com/code/kanncaa1/recurrent-neural-network-with-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tweedejaars_project import load_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tycho/miniconda3/envs/tweedejaars_project/lib/python3.12/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/home/tycho/miniconda3/envs/tweedejaars_project/lib/python3.12/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'Series.swapaxes' is deprecated and will be removed in a future version. Please use 'Series.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Prepare Dataset \"\"\"\n",
    "\n",
    "# load data\n",
    "df = load_df()\n",
    "highly_correlated_features = ['naive_strategy_action', 'min_ptu_price_known',\n",
    "                              'min_price_published', 'forecast_wind',\n",
    "                              'time_since_last_two_sided',\n",
    "                              'two_sided_daily_count', 'mid_price_published',\n",
    "                              'vwap_avg', 'vwap_std', 'vwap_qty_sum',\n",
    "                              'minute_in_ptu', 'downward_dispatch_published',\n",
    "                              'settlement_price_bestguess', 'import_capacity',\n",
    "                              'upward_dispatch_published',\n",
    "                              'settlement_price_realized', 'forecast_solar']\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "# split data into features and target\n",
    "targets_numpy = df['target_two_sided_ptu']\n",
    "features_numpy = df[highly_correlated_features]\n",
    "\n",
    "# train test split\n",
    "# could be replaced with scikit-learn TimeSeriesSplit\n",
    "X_train, X_test = np.split(features_numpy, [int(.7 * len(features_numpy))])\n",
    "y_train, y_test = np.split(targets_numpy, [int(.7 * len(targets_numpy))])\n",
    "\n",
    "# create feature and targets tensor for train and test set.\n",
    "featuresTrain = torch.Tensor(X_train.to_numpy(dtype=np.float64))\n",
    "targetsTrain = torch.Tensor(y_train.to_numpy(dtype=np.float64))\n",
    "\n",
    "featuresTest = torch.Tensor(X_test.to_numpy(dtype=np.float64))\n",
    "targetsTest = torch.Tensor(y_test.to_numpy(dtype=np.float64))\n",
    "\n",
    "# force the right dimension\n",
    "targetsTrain = targetsTrain.unsqueeze(1)\n",
    "targetsTest = targetsTest.unsqueeze(1)\n",
    "\n",
    "# Pytorch train and test sets\n",
    "train = torch.cat((featuresTrain, targetsTrain), dim=1)\n",
    "test = torch.cat((featuresTest, targetsTest), dim=1)\n",
    "\n",
    "# batch_size, epoch and iteration\n",
    "first_batch_size = 15\n",
    "n_iters = 100000\n",
    "num_epochs = int(n_iters / (len(X_train) / first_batch_size))\n",
    "\n",
    "# data loader makes batches and iterable\n",
    "train_loader = DataLoader(train, batch_size = first_batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test, batch_size = first_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create RNN Model \"\"\"\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # RNN\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers, batch_first=True,\n",
    "                          nonlinearity='relu')\n",
    "        \n",
    "        # Collection layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        # Range redistribution [0,1]\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.shape[0], self.hidden_dim,\n",
    "                         requires_grad=True)\n",
    "        \n",
    "        try:\n",
    "            h0 = hn\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # One time step\n",
    "        rnn_out, hn = self.rnn(x, h0)\n",
    "        \n",
    "        fc_out = self.fc(rnn_out[:, -1])  # last output\n",
    "\n",
    "        output = self.sigmoid(fc_out)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# Create RNN\n",
    "input_dim = 17    # input dimension\n",
    "hidden_dim = 60   # hidden layer dimension\n",
    "num_layers = 2    # number of hidden layers\n",
    "output_dim = 15   # output dimension\n",
    "seq_length = 15   # the number of time steps in each sequence\n",
    "\n",
    "model = RNNModel(input_dim, hidden_dim, num_layers, output_dim)\n",
    "\n",
    "# SGD Optimizer\n",
    "learning_rate = 0.05\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 20  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 30  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 40  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 50  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 60  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 70  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 80  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 90  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 100  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 110  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 120  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 130  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 140  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 150  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 160  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 170  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 180  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 190  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 200  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 210  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 220  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 230  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 240  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 250  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 260  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 270  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 280  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 290  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 300  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 310  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 320  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 330  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 340  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 350  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 360  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 370  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 380  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 390  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 400  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 410  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 420  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 430  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 440  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 450  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 460  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 470  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 480  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 490  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 500  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 510  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 520  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 530  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 540  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 550  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 560  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 570  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 580  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 590  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 600  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 610  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 620  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 630  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 640  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 650  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 660  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 670  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 680  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 690  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 700  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 710  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 720  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 730  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 740  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 750  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 760  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 770  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 780  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 790  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 800  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 810  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 820  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 830  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 840  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 850  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 860  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 870  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 880  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 890  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 900  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 910  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 920  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 930  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 940  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 950  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 960  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 970  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 980  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 990  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1000  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1010  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1020  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1030  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1040  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1050  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1060  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1070  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1080  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1090  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1100  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1110  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1120  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1130  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1140  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1150  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1160  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1170  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1180  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1190  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1200  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1210  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1220  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1230  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1240  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1250  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1260  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1270  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1280  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1290  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1300  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1310  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1320  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1330  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1340  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1350  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1360  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1370  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1380  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1390  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1400  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1410  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1420  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1430  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1440  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1450  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1460  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1470  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1480  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1490  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1500  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1510  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1520  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1530  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1540  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1550  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1560  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1570  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1580  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1590  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1600  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1610  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1620  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1630  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1640  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1650  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1660  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1670  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1680  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1690  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1700  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1710  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1720  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n",
      "Iteration: 1730  Loss: loss.data[0]  Accuracy: 52.77777862548828 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[419], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Forward propagation\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Get predictions from the maximum value\u001b[39;00m\n\u001b[1;32m     57\u001b[0m predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/tweedejaars_project/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tweedejaars_project/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[417], line 35\u001b[0m, in \u001b[0;36mRNNModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# One time step\u001b[39;00m\n\u001b[1;32m     33\u001b[0m rnn_out, hn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(x, h0)\n\u001b[0;32m---> 35\u001b[0m fc_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrnn_out\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# last output\u001b[39;00m\n\u001b[1;32m     37\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(fc_out)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/tweedejaars_project/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tweedejaars_project/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tweedejaars_project/lib/python3.12/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\" Run model \"\"\"\n",
    "\n",
    "batch_size = int(first_batch_size / seq_length) \n",
    "loss_list = []\n",
    "iteration_list = []\n",
    "accuracy_list = []\n",
    "count = 0\n",
    "error = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        \n",
    "        # print(batch.shape)\n",
    "        if batch.shape[0] != 15:\n",
    "            break\n",
    "\n",
    "        train = batch[:, :-1].reshape((batch_size, seq_length, input_dim))\n",
    "        targets = batch[:, -1:]\n",
    "\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward propagation\n",
    "        # print(batch_size, seq_length, input_dim)\n",
    "        # print(train.shape)\n",
    "        # print(batch[:1, :])\n",
    "        outputs = model(train)\n",
    "        \n",
    "        # Calculate softmax and cross entropy loss\n",
    "        # print(\"outputs\\n\",outputs)\n",
    "        # print(outputs.T.shape, targets.shape)\n",
    "        loss = error(outputs.T, targets)\n",
    "        \n",
    "        # Calculating gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "        # Calculate Accuracy\n",
    "        if count % 5 == 0:\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for batch in test_loader:\n",
    "                \n",
    "                if batch.shape[0] == 15:\n",
    "                    features = batch[:, :-1].reshape((batch_size, seq_length, input_dim))\n",
    "                    labels = batch[:,-1:]\n",
    "                    \n",
    "                    # Forward propagation\n",
    "                    outputs = model(features)\n",
    "                    \n",
    "                    # Get predictions from the maximum value\n",
    "                    predicted = torch.max(outputs.data, 1)[1]\n",
    "                    \n",
    "                    # Total number of labels\n",
    "                    total += labels.size(0)\n",
    "                    \n",
    "                    correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct / float(total)\n",
    "            \n",
    "            # store loss and iteration\n",
    "            loss_list.append(loss.data)\n",
    "            iteration_list.append(count)\n",
    "            accuracy_list.append(accuracy)\n",
    "            if count % 10 == 0:\n",
    "                # Print Loss\n",
    "                print(f'Iteration: {count}  Loss: {\"loss.data[0]\"}  Accuracy: {accuracy} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Model Evaluation \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweedejaars_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
