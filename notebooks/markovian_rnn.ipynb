{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model type: Markovian RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from tweedejaars_project import *\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"import_capacity\",\n",
    "    \"mid_price_published\",\n",
    "    \"upward_dispatch_published\",\n",
    "    \"downward_dispatch_published\",\n",
    "    'min_ptu_price_known',\n",
    "    \"max_ptu_price_known\",\n",
    "    \"settlement_price_bestguess\",\n",
    "    'PTU',\n",
    "    'forecast_wind',\n",
    "    'forecast_solar',\n",
    "    'forecast_demand',\n",
    "    'time_since_last_two_sided',\n",
    "    'two_sided_daily_count',\n",
    "    'ptu_id',\n",
    "    'naive_strategy_action',\n",
    "    'minute_in_ptu',\n",
    "    \"hvq_delta\",\n",
    "    \"residual_load\",\n",
    "    \"dispatch_diff\",\n",
    "    \"igcc_diff\",\n",
    "    \"is_balanced\",\n",
    "    \"weekday\",\n",
    "    \"workday\",\n",
    "    \"hour\",\n",
    "    \"month\",\n",
    "    \"day\",\n",
    "    \"minute\",\n",
    "\n",
    "]\n",
    "# already used\n",
    "target = 'fix_two_sided_ptu_alt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple forward filling of features when NaN values are in columns\n",
    "def interpolate_feature(df, features):\n",
    "    for feature in features:\n",
    "        df[feature] = df[feature].ffill()\n",
    "    return df\n",
    "\n",
    "\n",
    "df = interpolate_feature(df, \n",
    "                        ['forecast_wind', 'forecast_solar', \n",
    "                         'forecast_demand', \"upward_dispatch_published\", \n",
    "                         \"downward_dispatch_published\", \"vwap_avg\",\n",
    "                        \"forecast_wind_delta\", \"forecast_solar_delta\",\n",
    "                        \"forecast_demand_delta\", \"residual_load\",\n",
    "                        \"dispatch_diff\", \"igcc_diff\", 'hvq_delta' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace two features with more informative bool feature\n",
    "def difference_published(df, features):\n",
    "    values = [0., 1., 2., 3.]\n",
    "    conditions = [\n",
    "        (df[features[0]].notna() & df[features[1]].isna()),  # feature1 has value, feature2 is NaN\n",
    "        (df[features[0]].isna() & df[features[1]].notna()),  # feature1 is NaN, feature2 has value\n",
    "        (df[features[0]].isna() & df[features[1]].isna()),   # both feature1 and feature2 are NaN\n",
    "        (df[features[0]].notna() & df[features[1]].notna())  # both feature1 and feature2 have values\n",
    "    ]\n",
    "    df['publish_info'] = np.select(conditions, values)\n",
    "    return df\n",
    "\n",
    "df = difference_published(df, ['min_price_published', 'max_price_published'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values with 0 \n",
    "def fill_vals_0(df, features):\n",
    "    for feature in features:\n",
    "        df[feature] = df[feature].fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = fill_vals_0(df, ['min_ptu_price_known', 'max_ptu_price_known', 'settlement_price_bestguess'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "ptu_window = 10 # This is the PTU window amount, so how many PTU's we take for the history\n",
    "batch_size = 15000\n",
    "input_size = len(features)  # Input size == amount of Features\n",
    "hidden_size = 2\n",
    "num_layers = 2\n",
    "output_size = 1 # always 1 for bool output\n",
    "num_epochs = 1\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data in vars\n",
    "splits = get_splits(df, features, target)\n",
    "train_data = splits['train']\n",
    "valid_data = splits['valid']\n",
    "test_data = splits['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset class to handle the sequences for our needs in a good format\n",
    "    \"\"\"\n",
    "    def __init__(self, data, ptu_window, ptu_length=15):\n",
    "        self.data_in = pd.DataFrame(data['in']).astype(np.float32)\n",
    "        self.data_out = pd.Series(data['out']).astype(np.float32)\n",
    "        self.ptu_window = ptu_window\n",
    "        self.ptu_length = ptu_length\n",
    "\n",
    "        self.ptu_history = self.ptu_window * self.ptu_length  # Entire window\n",
    "        self.sequence_indices, self.sequence_lengths = self.create_sequences()\n",
    "        \n",
    "\n",
    "    # Create custom sequences for each row, so the 'history' added with the current row\n",
    "    def create_sequences(self):\n",
    "        sequence_indices, sequence_lengths = [], []\n",
    "\n",
    "        row_idx = 0  # Index of the current row\n",
    "        start_idx = 0  # Index of the furthest row in history\n",
    "\n",
    "        counter = self.ptu_length\n",
    "\n",
    "        for _ in range(len(self.data_in)):\n",
    "            sequence_indices.append((start_idx, row_idx))\n",
    "            sequence_lengths.append(row_idx - start_idx)  # Add length of sequence\n",
    "\n",
    "            # Start using the counter only if the row idx is more than the history length,\n",
    "            # for compatability\n",
    "            if row_idx >= self.ptu_history: \n",
    "                counter -= 1\n",
    "                if counter == 0:\n",
    "                    start_idx += self.ptu_length\n",
    "                    counter = self.ptu_length\n",
    "\n",
    "            row_idx += 1\n",
    "\n",
    "        return sequence_indices, sequence_lengths\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequence_indices)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get idx's\n",
    "        start_idx, row_idx = self.sequence_indices[idx]\n",
    "        length = self.sequence_lengths[idx]\n",
    "\n",
    "        # np array of sequence and target \n",
    "        sequence = self.data_in.iloc[start_idx:row_idx + 1].values  \n",
    "        target = self.data_out.iloc[row_idx]\n",
    "\n",
    "        # flip sequence for correct order\n",
    "        sequence = torch.tensor(sequence, dtype=torch.float32).flip(0)  \n",
    "\n",
    "        # Pad for the first row idx's if it is less than the history length,\n",
    "        if length <= self.ptu_history and start_idx == 0:\n",
    "            sequence = F.pad(sequence, (0, 0, 0, (self.ptu_history - length) + self.ptu_length -1), mode='constant', value=-np.inf)\n",
    "        \n",
    "        # Dynamic padding for in the current ptu if the row is not at the last idx of the ptu\n",
    "        else:\n",
    "            current_seq_len = self.ptu_length - ((row_idx - start_idx) % self.ptu_length)\n",
    "            sequence = F.pad(sequence, (0, 0, 0, current_seq_len - 1), mode='constant', value=-np.inf)\n",
    "        \n",
    "        return sequence, torch.tensor(target, dtype=torch.float32), length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(MarkovModel, self).__init__()\n",
    "        # SImple rnn with dropout\n",
    "        self.lstm = nn.RNN(input_size, hidden_size, num_layers, dropout=0.5, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # Special function to let the model know there are padded rows\n",
    "        packed_input = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, _ = self.lstm(packed_input)\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        # Use the last valid output for each sequence\n",
    "        idx = (lengths - 1).view(-1, 1, 1).expand(output.size(0), 1, output.size(2))\n",
    "        output = output.gather(1, idx).squeeze(1)\n",
    "        output = self.fc(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and move it to the selected device\n",
    "\n",
    "model = MarkovModel(input_size, hidden_size, num_layers, output_size)\n",
    "\n",
    "# Create tensor for the target data and calculate class weights for imbalanced classes\n",
    "tensor_target = torch.tensor(train_data['out']).float()\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=(len(tensor_target)/ tensor_target.sum())*0.43)\n",
    "\n",
    "# Initialize optimizer with AdamW and AMSGrad variant\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, amsgrad=True, weight_decay=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data \n",
    "def prepare_data_loader(data, ptu_window, batch_size):\n",
    "    dataset = TimeSeriesDataset(data, ptu_window)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = prepare_data_loader(train_data, ptu_window, batch_size)\n",
    "valid_loader = prepare_data_loader(valid_data, ptu_window, batch_size)\n",
    "test_loader = prepare_data_loader(test_data, ptu_window, batch_size)\n",
    "\n",
    "print(f'expected batches training: {len(train_loader)}')\n",
    "print(f'expected batches validation: {len(valid_loader)}')\n",
    "print(f'expected batches testing: {len(test_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop for the RNN\n",
    "def train_rnn(model, train_loader, val_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        # Training phase\n",
    "        for batch_idx, (sequences, targets, lengths) in enumerate(train_loader):\n",
    "            \n",
    "            print(f\"Training batch {batch_idx}, Sequence shape: {sequences.shape}, Lengths: {lengths}\")\n",
    "\n",
    "            sequences = sequences.float()\n",
    "            targets = targets.float().view(-1, 1)\n",
    "            lengths += 1\n",
    "            lengths = lengths.to(torch.int64).cpu()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(sequences, lengths)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (sequences, targets, lengths) in enumerate(val_loader):\n",
    "\n",
    "                print(f\"Validation batch {batch_idx}, Sequence shape: {sequences.shape}, Lengths: {lengths}\")\n",
    "\n",
    "                sequences = sequences.float()\n",
    "                targets = targets.float().view(-1, 1)\n",
    "                lengths += 1\n",
    "                lengths = lengths.to(torch.int64).cpu()\n",
    "\n",
    "                outputs = model(sequences, lengths)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # Switch back to training mode\n",
    "        model.train()\n",
    "        \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, val_losses = train_rnn(model, train_loader, valid_loader, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over all Batches')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Validation Loss Over all Batches')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "save_name = 'markovian_model_1'\n",
    "folder_name = 'rnn'\n",
    "\n",
    "# Uncomment to save the model\n",
    "# save_model(model, save_name, folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing loop\n",
    "def test_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_outputs = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (sequences, targets, lengths) in enumerate(data_loader):\n",
    "            print(f\"batch {batch_idx}, Sequence shape: {sequences.shape}, Lengths: {lengths}\")\n",
    "\n",
    "            sequences = sequences.float()\n",
    "            targets = targets.float()\n",
    "            lengths += 1\n",
    "\n",
    "            # Ensure lengths is a 1D CPU int64 tensor\n",
    "            lengths = lengths.to(torch.int64).cpu()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(sequences, lengths)\n",
    "            all_outputs.append(outputs)\n",
    "            all_targets.append(targets)\n",
    "            \n",
    "    return torch.cat(all_outputs), torch.cat(all_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place to test model\n",
    "\n",
    "# change these two variables correctly to test different sets\n",
    "chosen_data_loader = valid_loader\n",
    "unaltered_df = valid_data['df']\n",
    "\n",
    "outputs, targets = test_model(model, valid_loader)\n",
    "probabilities = torch.sigmoid(outputs)\n",
    "# threshholding to create bolean predictions\n",
    "predictions = (probabilities > 0.5).float()\n",
    "\n",
    "# choose correct df for the metric\n",
    "recasted_pred = recast_pred(predictions.flatten())\n",
    "show_metrics_adjusted(unaltered_df, recasted_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweedejaars_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
