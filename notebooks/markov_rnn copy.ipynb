{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from tweedejaars_project import *\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.tree import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'import_capacity',\n",
    "    # 'min_price_published',\n",
    "    'mid_price_published',\n",
    "    # 'max_price_published',\n",
    "    # 'min_ptu_price_known',\n",
    "    # 'max_ptu_price_known',\n",
    "    # 'settlement_price_bestguess',\n",
    "    'time_since_last_two_sided',\n",
    "    'two_sided_daily_count',\n",
    "    'PTU',\n",
    "    'naive_strategy_action',\n",
    "    'forecast_wind',\n",
    "    'forecast_solar',\n",
    "    'forecast_demand',\n",
    "    'ptu_id',\n",
    "    'fix_two_sided_ptu_realtime'\n",
    "]\n",
    "\n",
    "# already used\n",
    "target = 'target_two_sided_ptu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_df()\n",
    "splits = get_splits(df, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0            0\n",
      "1            0\n",
      "2            0\n",
      "3            0\n",
      "4            0\n",
      "          ... \n",
      "123835    8255\n",
      "123836    8255\n",
      "123837    8255\n",
      "123838    8255\n",
      "123839    8255\n",
      "Name: ptu_id, Length: 123840, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "sequence_length = 5 # Dit is het PTU aantal dus ptu_id's niet row aantal\n",
    "batch_size = 15000\n",
    "input_size = len(features)\n",
    "hidden_size = 16\n",
    "num_layers = 2\n",
    "output_size = 1\n",
    "num_epochs = 1\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "# Split the data in vars\n",
    "train_data = splits['train']\n",
    "# test_data = splits['test']\n",
    "\n",
    "rows = 20\n",
    "print(train_data['in']['ptu_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, seq_length, ptu_length=15, nan_val1 = 100000, nan_val2 = -100000):\n",
    "        self.data_in = data['in'].astype(np.float32)\n",
    "        self.data_out = data['out'].astype(np.float32)\n",
    "        self.seq_length = seq_length\n",
    "        self.ptu_length = ptu_length\n",
    "        self.max_length = (self.seq_length * self.ptu_length)\n",
    "        self.data_in_padded = self.create_padding()\n",
    "\n",
    "        self.sequence_indices = self.create_sequences()\n",
    "\n",
    "        # # DIT IS EEN HACK DUS WSS VERANDEREN\n",
    "        # self.data_in = self.data_in.fillna({'min_price_published':nan_val1, 'max_price_published':nan_val2}).astype(np.float32)\n",
    "\n",
    "    def create_padding(self, padding_value=np.nan):\n",
    "        new_rows = pd.DataFrame([[padding_value] * len(self.data_in.columns)] * self.max_length, columns=self.data_in.columns)\n",
    "        return pd.concat([new_rows, self.data_in], ignore_index=True)\n",
    "\n",
    "    def create_sequences(self):\n",
    "        sequence_indices = []        \n",
    "        first_idx = 0\n",
    "        last_idx = self.max_length\n",
    "\n",
    "        counter = self.ptu_length\n",
    "        for _ in range(len(self.data_in)):\n",
    "            sequence_indices.append((first_idx, last_idx))\n",
    "            counter -= 1\n",
    "            if counter == 0:\n",
    "                first_idx += self.ptu_length\n",
    "                counter = self.ptu_length\n",
    "            \n",
    "            last_idx += 1\n",
    "\n",
    "        return sequence_indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequence_indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start_idx, end_idx = self.sequence_indices[idx]\n",
    "        \n",
    "        # np array sequence and target\n",
    "        sequence = self.data_in_padded.iloc[start_idx:end_idx].values\n",
    "        target = self.data_out.iloc[idx]\n",
    "\n",
    "        sequence = torch.tensor(sequence, dtype=torch.float32)        \n",
    "        current_seq_len = self.ptu_length - ((end_idx - start_idx) % self.ptu_length)\n",
    "        corrected_sequence = F.pad(sequence, (0, 0, 0, current_seq_len), mode='constant', value=np.nan)\n",
    "                \n",
    "        return corrected_sequence, torch.tensor(target, dtype=torch.float32)\n",
    "\n",
    "    \n",
    "    # def get_all_sequences(self):\n",
    "    #     all_sequences = []\n",
    "    #     all_targets = []\n",
    "    #     for index in range(len(self.data_in)):\n",
    "    #         sequence, target = self.__getitem__(index)\n",
    "    #         all_sequences.append(sequence)\n",
    "    #         all_targets.append(target)\n",
    "    #     return all_sequences, all_targets\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        packed_input = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, _ = self.rnn(packed_input)\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        # Use the last valid output for each sequence\n",
    "        idx = (lengths - 1).view(-1, 1).expand(output.size(0), output.size(2)).unsqueeze(1)\n",
    "        output = output.gather(1, idx).squeeze(1)\n",
    "        \n",
    "        output = self.fc(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_loader(data, sequence_length, batch_size):\n",
    "    dataset = TimeSeriesDataset(data, sequence_length)\n",
    "    # all_sequences, all_targets = dataset.get_all_sequences()\n",
    "    print('done')\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6872/3523582104.py:17: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat([new_rows, self.data_in], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "train_loader = prepare_data_loader(train_data, sequence_length, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0\n",
      "Sequence shape: torch.Size([15000, 90, 11])\n",
      "Targets shape: torch.Size([15000])\n",
      "Batch: 1\n",
      "Sequence shape: torch.Size([15000, 90, 11])\n",
      "Targets shape: torch.Size([15000])\n",
      "Batch: 2\n",
      "Sequence shape: torch.Size([15000, 90, 11])\n",
      "Targets shape: torch.Size([15000])\n",
      "Batch: 3\n",
      "Sequence shape: torch.Size([15000, 90, 11])\n",
      "Targets shape: torch.Size([15000])\n",
      "Batch: 4\n",
      "Sequence shape: torch.Size([15000, 90, 11])\n",
      "Targets shape: torch.Size([15000])\n",
      "Batch: 5\n",
      "Sequence shape: torch.Size([15000, 90, 11])\n",
      "Targets shape: torch.Size([15000])\n",
      "Batch: 6\n",
      "Sequence shape: torch.Size([15000, 90, 11])\n",
      "Targets shape: torch.Size([15000])\n",
      "Batch: 7\n",
      "Sequence shape: torch.Size([15000, 90, 11])\n",
      "Targets shape: torch.Size([15000])\n",
      "Batch: 8\n",
      "Sequence shape: torch.Size([3840, 90, 11])\n",
      "Targets shape: torch.Size([3840])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (sequences, targets) in enumerate(train_loader):\n",
    "    print(\"Batch:\", batch_idx)\n",
    "    print(\"Sequence shape:\", sequences.shape)  # Print the shape of the input sequences\n",
    "    print(\"Targets shape:\", targets.shape)  # Print the shape of the targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Sequence shape: torch.Size([15000, 90, 11]), Lengths: tensor([90, 90, 90,  ..., 90, 90, 90])\n",
      "Batch 1, Sequence shape: torch.Size([15000, 90, 11]), Lengths: tensor([90, 90, 90,  ..., 90, 90, 90])\n",
      "Batch 2, Sequence shape: torch.Size([15000, 90, 11]), Lengths: tensor([90, 90, 90,  ..., 90, 90, 90])\n",
      "Batch 3, Sequence shape: torch.Size([15000, 90, 11]), Lengths: tensor([90, 90, 90,  ..., 90, 90, 90])\n",
      "Batch 4, Sequence shape: torch.Size([15000, 90, 11]), Lengths: tensor([90, 90, 90,  ..., 90, 90, 90])\n",
      "Batch 5, Sequence shape: torch.Size([15000, 90, 11]), Lengths: tensor([90, 90, 90,  ..., 90, 90, 90])\n",
      "Batch 6, Sequence shape: torch.Size([15000, 90, 11]), Lengths: tensor([90, 90, 90,  ..., 90, 90, 90])\n",
      "Batch 7, Sequence shape: torch.Size([15000, 90, 11]), Lengths: tensor([90, 90, 90,  ..., 90, 90, 90])\n",
      "Batch 8, Sequence shape: torch.Size([3840, 90, 11]), Lengths: tensor([90, 90, 90,  ..., 90, 90, 90])\n"
     ]
    }
   ],
   "source": [
    "def train_rnn(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, (sequences, targets) in enumerate(train_loader):\n",
    "            # Prepare the data\n",
    "            sequences = sequences.float()\n",
    "            targets = targets.float()\n",
    "            lengths = torch.tensor([len(seq) for seq in sequences])\n",
    "            \n",
    "            # Debugging print statements\n",
    "            print(f\"Batch {batch_idx}, Sequence shape: {sequences.shape}, Lengths: {lengths}\")\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(sequences, lengths)\n",
    "            \n",
    "            # Compute loss\n",
    "            targets = targets.unsqueeze(1)  # Ensure targets have the shape (batch_size, 1)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "model = RNNModel(input_size, hidden_size, num_layers, output_size)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_rnn(model, train_loader, criterion, optimizer, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, 'markov', 'rnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.9553\n"
     ]
    }
   ],
   "source": [
    "def test_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_outputs = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for sequences, targets in data_loader:\n",
    "            sequences = sequences.float()\n",
    "            targets = targets.float()\n",
    "            lengths = torch.tensor([len(seq) for seq in sequences])\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(sequences, lengths)\n",
    "            all_outputs.append(outputs)\n",
    "            all_targets.append(targets)\n",
    "            \n",
    "    return torch.cat(all_outputs), torch.cat(all_targets)\n",
    "\n",
    "loaded_model = load_model('markov', 'rnn')\n",
    "\n",
    "# Test the model on the training set\n",
    "outputs, targets = test_model(loaded_model, train_loader)\n",
    "\n",
    "# Convert outputs to probabilities\n",
    "probabilities = torch.sigmoid(outputs)\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "predictions = (probabilities > 0.5).float()\n",
    "\n",
    "# Evaluate the predictions\n",
    "accuracy = (predictions == targets.unsqueeze(1)).float().mean()\n",
    "print(f'Accuracy on training set: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RNNModel(input_size, hidden_size, num_layers, output_size)\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch_idx, (sequences, targets) in enumerate(train_loader):\n",
    "#     print(\"Batch:\", batch_idx)\n",
    "#     print(\"Sequence shape:\", sequences.shape)  # Print the shape of the input sequences\n",
    "#     print(\"Targets shape:\", targets.shape)  # Print the shape of the targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training loop (placeholder, implement training logic)\n",
    "# for epoch in range(1):\n",
    "#     for sequences, targets in train_loader:\n",
    "#         lengths = [min(len(seq), sequence_length) for seq in sequences]\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(sequences, lengths)\n",
    "#         loss = criterion(outputs, targets)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweedejaars_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
